# Part 12: Industry Projects

## Table of Contents

1. [Introduction](#introduction)
2. [Project 1: Enterprise RAG System](#project-1-enterprise-rag)
3. [Project 2: AI Customer Support Agent](#project-2-customer-support)
4. [Project 3: Code Review Assistant](#project-3-code-review)
5. [Project 4: Document Intelligence System](#project-4-document-intelligence)
6. [Project 5: Autonomous AI Agent](#project-5-autonomous-agent)
7. [Project 6: Multimodal Search Engine](#project-6-multimodal-search)
8. [Project 7: Natural Language to SQL](#project-7-nl-to-sql)
9. [Project 8: Content Moderation System](#project-8-content-moderation)

---

## Introduction {#introduction}

### About These Projects

**These are complete, production-ready project architectures.**

Each project includes:

- Problem statement & requirements
- Architecture diagram
- Complete tech stack
- Step-by-step implementation
- Challenges & solutions
- Evaluation strategy
- Interview discussion points

**Target audience:**

- Junior engineers: Study architectures, understand components
- Mid engineers: Implement 1-2 projects for portfolio
- Senior engineers: Reference for system design interviews

---

## Project 1: Enterprise RAG System {#project-1-enterprise-rag}

### Problem Statement

**Build internal knowledge base for 1000-employee company with 100K+ documents.**

**Requirements:**

- Semantic search across all documents (PDFs, Word, PowerPoint, Confluence)
- User permission filtering (users only see docs they have access to)
- Hybrid search (keyword + semantic)
- Re-ranking for accuracy
- Usage analytics (what's searched, what's clicked)
- <2s response time (p95)
- <$0.01 per query cost

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Users (1000)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Load Balancer (NGINX)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Server 1   â”‚ â”‚  API Server 2   â”‚ â”‚  API Server 3   â”‚
â”‚   (FastAPI)     â”‚ â”‚   (FastAPI)     â”‚ â”‚   (FastAPI)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚                   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                   â”‚                   â”‚
         â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BM25 Search    â”‚ â”‚  Vector Search  â”‚ â”‚  Re-ranker      â”‚
â”‚  (Elasticsearch)â”‚ â”‚  (Pinecone)     â”‚ â”‚  (Cohere)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚                   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  LLM Generation â”‚
                    â”‚    (GPT-4o)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL     â”‚ â”‚  Redis Cache    â”‚ â”‚  S3 Storage     â”‚
â”‚  (Metadata)     â”‚ â”‚  (Query Cache)  â”‚ â”‚  (Raw Docs)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Monitoring     â”‚
â”‚  (Datadog)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tech Stack

```yaml
Infrastructure:
  - Cloud: AWS
  - Container: Docker + Kubernetes
  - Load Balancer: NGINX

Backend:
  - Framework: FastAPI (Python)
  - Authentication: OAuth2 + JWT

Search:
  - Keyword: Elasticsearch
  - Semantic: Pinecone (100K docs, 768-dim embeddings)
  - Re-ranker: Cohere Rerank API

LLM:
  - Model: GPT-4o (cost-effective, fast)
  - Embeddings: text-embedding-3-small (512-dim)

Storage:
  - Metadata: PostgreSQL
  - Cache: Redis
  - Documents: AWS S3

Monitoring:
  - Logs: Datadog
  - Metrics: Prometheus + Grafana
```

### Implementation

**Step 1: Document Ingestion**

```python
from unstructured.partition.auto import partition
from langchain.text_splitter import RecursiveCharacterTextSplitter
import hashlib

def ingest_document(file_path, user_permissions):
    # Extract text
    elements = partition(filename=file_path)
    text = "\n\n".join([e.text for e in elements])

    # Chunk
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
    )
    chunks = splitter.split_text(text)

    # Generate embeddings
    embeddings = openai.Embedding.create(
        model="text-embedding-3-small",
        input=chunks,
    )

    # Store in Pinecone (vector DB)
    vectors = []
    for i, (chunk, emb) in enumerate(zip(chunks, embeddings.data)):
        doc_id = hashlib.sha256(f"{file_path}_{i}".encode()).hexdigest()

        vectors.append({
            "id": doc_id,
            "values": emb.embedding,
            "metadata": {
                "text": chunk,
                "source": file_path,
                "chunk_index": i,
                "permissions": user_permissions,  # ["engineering", "all"]
            },
        })

    pinecone_index.upsert(vectors)

    # Index in Elasticsearch (keyword search)
    for chunk in chunks:
        es.index(
            index="documents",
            document={
                "text": chunk,
                "source": file_path,
                "permissions": user_permissions,
            },
        )

    # Store metadata in PostgreSQL
    db.execute("""
        INSERT INTO documents (file_path, num_chunks, permissions, uploaded_at)
        VALUES (%s, %s, %s, NOW())
    """, (file_path, len(chunks), user_permissions))
```

**Step 2: Hybrid Search**

```python
def hybrid_search(query, user_groups, top_k=20):
    # Semantic search (Pinecone)
    query_embedding = openai.Embedding.create(
        model="text-embedding-3-small",
        input=[query],
    ).data[0].embedding

    semantic_results = pinecone_index.query(
        vector=query_embedding,
        top_k=top_k,
        filter={"permissions": {"$in": user_groups + ["all"]}},
        include_metadata=True,
    )

    # Keyword search (Elasticsearch)
    keyword_results = es.search(
        index="documents",
        query={
            "bool": {
                "must": {"match": {"text": query}},
                "filter": {"terms": {"permissions": user_groups + ["all"]}},
            }
        },
        size=top_k,
    )

    # Combine and deduplicate
    combined = {}

    for match in semantic_results.matches:
        combined[match.id] = {
            "text": match.metadata["text"],
            "score": match.score,
            "source": "semantic",
        }

    for hit in keyword_results["hits"]["hits"]:
        doc_id = hit["_id"]
        if doc_id in combined:
            # Average scores if both found it
            combined[doc_id]["score"] = (combined[doc_id]["score"] + hit["_score"]) / 2
        else:
            combined[doc_id] = {
                "text": hit["_source"]["text"],
                "score": hit["_score"],
                "source": "keyword",
            }

    # Sort by score
    results = sorted(combined.values(), key=lambda x: x["score"], reverse=True)

    return results[:top_k]
```

**Step 3: Re-ranking**

```python
import cohere

co = cohere.Client(api_key="YOUR_KEY")

def rerank(query, documents, top_n=5):
    # Cohere rerank
    results = co.rerank(
        model="rerank-english-v3.0",
        query=query,
        documents=[doc["text"] for doc in documents],
        top_n=top_n,
    )

    # Return top N
    reranked = [documents[r.index] for r in results.results]
    return reranked
```

**Step 4: RAG Generation**

```python
def rag_query(query, user_groups):
    # Check cache
    cache_key = hashlib.sha256(query.encode()).hexdigest()
    cached = redis.get(cache_key)
    if cached:
        return json.loads(cached)

    # Hybrid search
    search_results = hybrid_search(query, user_groups, top_k=20)

    # Re-rank
    top_docs = rerank(query, search_results, top_n=5)

    # Build context
    context = "\n\n".join([f"[{i+1}] {doc['text']}" for i, doc in enumerate(top_docs)])

    # Generate answer
    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant. Answer based on provided context. Cite sources using [1], [2], etc.",
            },
            {
                "role": "user",
                "content": f"Context:\n{context}\n\nQuestion: {query}\n\nAnswer:",
            },
        ],
        temperature=0,
    )

    answer = response.choices[0].message.content

    # Cache result (1 hour)
    redis.setex(cache_key, 3600, json.dumps({"answer": answer, "sources": top_docs}))

    # Log usage
    db.execute("""
        INSERT INTO queries (query, user_group, num_results, latency_ms, timestamp)
        VALUES (%s, %s, %s, %s, NOW())
    """, (query, user_groups[0], len(top_docs), latency_ms))

    return {"answer": answer, "sources": top_docs}
```

**Step 5: FastAPI Server**

```python
from fastapi import FastAPI, Depends, HTTPException
from fastapi.security import OAuth2PasswordBearer
from pydantic import BaseModel

app = FastAPI()
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

class QueryRequest(BaseModel):
    query: str

class QueryResponse(BaseModel):
    answer: str
    sources: list

def get_current_user(token: str = Depends(oauth2_scheme)):
    # Decode JWT token
    user = decode_jwt(token)
    if not user:
        raise HTTPException(status_code=401, detail="Invalid token")
    return user

@app.post("/query", response_model=QueryResponse)
async def query_endpoint(request: QueryRequest, user=Depends(get_current_user)):
    # Get user groups (for permission filtering)
    user_groups = db.fetch_user_groups(user.id)

    # Query
    result = rag_query(request.query, user_groups)

    return QueryResponse(
        answer=result["answer"],
        sources=[{"text": s["text"][:200], "score": s["score"]} for s in result["sources"]],
    )

@app.get("/health")
async def health():
    return {"status": "ok"}
```

### Challenges & Solutions

**Challenge 1: Permission filtering at scale**

**Problem:** 100K docs Ã— 1000 users = complex permissions.

**Solution:**

- Store permissions as metadata in vector DB
- Filter at query time using Pinecone filters
- Cache user permissions in Redis (reduce DB queries)

**Challenge 2: Cost ($0.01 per query target)**

**Cost breakdown:**

```
Embedding query: $0.00001
Pinecone search: $0.00005
Cohere rerank: $0.002
GPT-4o generation: $0.005
Total: $0.0071 per query âœ… (Under budget)
```

**Optimization:**

- Cache frequent queries (50% hit rate â†’ 50% savings)
- Use text-embedding-3-small (cheaper than large)
- Only rerank top 20 (not all semantic results)

**Challenge 3: Latency (<2s target)**

**Latency breakdown:**

```
Hybrid search: 200ms
Rerank: 300ms
LLM generation: 1200ms
Total: 1700ms âœ… (Under 2s)
```

**Optimization:**

- Parallel hybrid search (semantic + keyword in parallel)
- Streaming LLM response (first token in 400ms)
- Redis caching (cached queries: 50ms)

### Evaluation

**Metrics:**

1. **Retrieval accuracy:**
   - Precision@5: 0.85 (85% of top 5 docs are relevant)
   - MRR: 0.78 (first relevant doc at position 1.3 on average)

2. **Answer quality:**
   - Human eval (100 queries): 4.2/5 rating
   - Hallucination rate: 3% (RAGAS faithfulness)

3. **Business metrics:**
   - Usage: 500 queries/day
   - User satisfaction (CSAT): 4.5/5
   - Time saved: 30 min/employee/week (was manual search)

4. **Performance:**
   - p95 latency: 1.8s âœ…
   - Cache hit rate: 48%
   - Uptime: 99.7%

5. **Cost:**
   - $0.007/query Ã— 500 queries/day = $3.50/day = $105/month âœ…

### Interview Discussion Points

**For mid-level:**

- "Why hybrid search instead of just semantic?"
  - Semantic misses exact keywords (e.g., "AWS SDK" vs "Amazon Web Services SDK")
  - Keyword misses synonyms
  - Hybrid combines strengths

- "How do you handle user permissions?"
  - Store as metadata in vector DB
  - Filter at query time
  - Trade-off: Slightly slower vs secure

**For senior-level:**

- "How would you scale to 1M documents?"
  - Pinecone scales to millions (serverless auto-scales)
  - Elasticsearch clusters (horizontal scaling)
  - Consider namespace partitioning (10Ã— namespaces)

- "How to reduce cost from $0.007 to $0.001 per query?"
  - Use smaller LLM (gpt-4o-mini: 10Ã— cheaper)
  - Increase cache TTL (1hr â†’ 24hr for stable content)
  - Batch embedding generation
  - Self-hosted reranker (cross-encoder model)

- "What if rerank API is down?"
  - Graceful degradation: Use hybrid search results directly
  - Alert team, but don't block user
  - Monitor rerank uptime SLA

---

## Project 2: AI Customer Support Agent {#project-2-customer-support}

### Problem Statement

**Build AI chatbot to handle 90% of customer support queries without human intervention.**

**Requirements:**

- Multi-turn conversations with memory
- Tool use: search knowledge base, create ticket, check order status, process refund
- Escalate to human if confidence <70% or user requests
- <2s response time
- <$0.01 per conversation cost
- Handle 10K conversations/day

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         User (Chat Widget)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Intent Classifier (gpt-4o-mini)        â”‚
â”‚  (Billing / Technical / General / Human)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â–¼           â–¼           â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Billing  â”‚ â”‚Technical â”‚ â”‚ General  â”‚ â”‚   Human     â”‚
â”‚  Agent   â”‚ â”‚  Agent   â”‚ â”‚  Agent   â”‚ â”‚ Escalation  â”‚
â”‚ (GPT-4o) â”‚ â”‚ (GPT-4o) â”‚ â”‚(gpt-4o-  â”‚ â”‚ (Create     â”‚
â”‚          â”‚ â”‚          â”‚ â”‚  mini)   â”‚ â”‚  Ticket)    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚            â”‚            â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â–¼            â–¼            â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Search   â”‚ â”‚  Check   â”‚ â”‚  Create  â”‚ â”‚   Refund     â”‚
â”‚   KB     â”‚ â”‚  Order   â”‚ â”‚  Ticket  â”‚ â”‚   (Requires  â”‚
â”‚(Pinecone)â”‚ â”‚  (API)   â”‚ â”‚  (DB)    â”‚ â”‚   Approval)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Conversation Memory (Redis)               â”‚
â”‚    User context, history (last 10 messages)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tech Stack

```yaml
Frontend:
  - Widget: React + Socket.IO (real-time)

Backend:
  - Framework: FastAPI (Python)
  - WebSocket: Socket.IO

LLMs:
  - Intent: gpt-4o-mini ($0.15/1M, fast)
  - Agents: gpt-4o ($2.50/1M, accurate)
  - Fallback: gpt-4o-mini (if budget limit)

Memory:
  - Redis (conversation history)
  - PostgreSQL (tickets, analytics)

Tools:
  - Knowledge Base: Pinecone
  - Order API: Internal REST API
  - Ticketing: Jira API
```

### Implementation

**Step 1: Intent Classification**

```python
def classify_intent(message, conversation_history):
    prompt = f"""
Conversation history:
{conversation_history}

User message: {message}

Classify intent:
- billing: Payment, refund, invoice questions
- technical: Product not working, bugs, how-to
- general: FAQs, greetings, general info
- human: User explicitly asks for human ("talk to agent")

Output JSON: {{"intent": "...", "confidence": 0-1}}
"""

    response = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"},
    )

    return json.loads(response.choices[0].message.content)
```

**Step 2: Specialized Agents**

```python
BILLING_AGENT_PROMPT = """
You are a billing support specialist.

Available tools:
- search_knowledge_base: Search internal docs
- get_invoice: Get user's invoice details
- process_refund: Initiate refund (requires confirmation)

Always:
1. Be empathetic
2. Explain clearly
3. Confirm before refunds
"""

def billing_agent(message, user_id, conversation_history):
    tools = [
        {
            "type": "function",
            "function": {
                "name": "search_knowledge_base",
                "parameters": {
                    "type": "object",
                    "properties": {"query": {"type": "string"}},
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "get_invoice",
                "parameters": {
                    "type": "object",
                    "properties": {"user_id": {"type": "string"}},
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "process_refund",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "user_id": {"type": "string"},
                        "amount": {"type": "number"},
                        "reason": {"type": "string"},
                    },
                },
            },
        },
    ]

    messages = [
        {"role": "system", "content": BILLING_AGENT_PROMPT},
    ]

    # Add conversation history
    for msg in conversation_history[-10:]:  # Last 10 messages
        messages.append(msg)

    messages.append({"role": "user", "content": message})

    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=messages,
        tools=tools,
    )

    # Handle tool calls
    if response.choices[0].message.tool_calls:
        tool_call = response.choices[0].message.tool_calls[0]

        if tool_call.function.name == "search_knowledge_base":
            args = json.loads(tool_call.function.arguments)
            result = search_kb(args["query"])
            return {"response": result, "tool_used": "search"}

        elif tool_call.function.name == "get_invoice":
            invoice = get_user_invoice(user_id)
            return {"response": f"Your last invoice: ${invoice['amount']} on {invoice['date']}", "tool_used": "invoice"}

        elif tool_call.function.name == "process_refund":
            args = json.loads(tool_call.function.arguments)
            # Require human approval for refunds
            return {
                "response": "To process this refund, I need to escalate to my supervisor. They'll contact you within 1 hour.",
                "escalate": True,
                "reason": "refund_approval",
            }

    return {"response": response.choices[0].message.content}
```

**Step 3: Escalation Logic**

```python
def should_escalate(agent_response, confidence):
    # Escalate if:
    # 1. Confidence < 70%
    # 2. Agent explicitly requests escalation
    # 3. User asks for human
    # 4. Sensitive actions (refunds)

    if confidence < 0.7:
        return True, "low_confidence"

    if agent_response.get("escalate"):
        return True, agent_response.get("reason", "agent_request")

    if "talk to human" in agent_response["response"].lower():
        return True, "user_request"

    return False, None

def escalate_to_human(user_id, conversation_history, reason):
    # Create ticket
    ticket = jira.create_issue(
        project="SUPPORT",
        summary=f"Escalation: {reason}",
        description=f"User: {user_id}\n\nConversation:\n{format_history(conversation_history)}",
        priority="High",
    )

    # Notify user
    return f"I've created a ticket for you (#{ticket.key}). A human agent will reach out within 1 hour."
```

**Step 4: Conversation Memory**

```python
def save_conversation(user_id, role, content):
    # Store in Redis (expire after 24 hours)
    key = f"conversation:{user_id}"
    message = {"role": role, "content": content, "timestamp": time.time()}

    redis.lpush(key, json.dumps(message))
    redis.expire(key, 86400)  # 24 hours

def get_conversation_history(user_id, limit=10):
    key = f"conversation:{user_id}"
    messages = redis.lrange(key, 0, limit - 1)

    return [json.loads(m) for m in messages]
```

**Step 5: Main Chatbot Logic**

```python
@app.websocket("/chat")
async def chat_endpoint(websocket: WebSocket):
    await websocket.accept()
    user_id = authenticate_websocket(websocket)

    while True:
        # Receive message
        message = await websocket.receive_text()

        # Get conversation history
        history = get_conversation_history(user_id)

        # Classify intent
        intent_result = classify_intent(message, history)
        intent = intent_result["intent"]
        confidence = intent_result["confidence"]

        # Route to agent
        if intent == "human" or confidence < 0.5:
            response = escalate_to_human(user_id, history, "low_confidence")
        elif intent == "billing":
            agent_response = billing_agent(message, user_id, history)
        elif intent == "technical":
            agent_response = technical_agent(message, user_id, history)
        else:
            agent_response = general_agent(message, user_id, history)

        # Check escalation
        if not isinstance(agent_response, str):
            escalate, reason = should_escalate(agent_response, confidence)
            if escalate:
                response = escalate_to_human(user_id, history, reason)
            else:
                response = agent_response["response"]

        # Save conversation
        save_conversation(user_id, "user", message)
        save_conversation(user_id, "assistant", response)

        # Send response
        await websocket.send_text(json.dumps({"message": response}))

        # Log metrics
        db.execute("""
            INSERT INTO chat_logs (user_id, intent, confidence, response_time, escalated)
            VALUES (%s, %s, %s, %s, %s)
        """, (user_id, intent, confidence, response_time, escalate))
```

### Challenges & Solutions

**Challenge 1: 90% automation target**

**Baseline:** 60% automation with simple rules.

**Improvements:**

- Intent classification: 60% â†’ 75% (Reduced mis-routing)
- Specialized agents: 75% â†’ 85% (Better domain knowledge)
- Confidence-based escalation: 85% â†’ 90% (Escalate uncertain cases)

**Result: 90% automation âœ…**

**Challenge 2: Cost (<$0.01 per conversation)**

```
Average conversation: 6 messages (3 user, 3 assistant)

Cost breakdown:
- Intent classification: 3 Ã— $0.00001 (gpt-4o-mini) = $0.00003
- Agent responses: 3 Ã— $0.003 (gpt-4o) = $0.009
- Total: $0.00903 per conversation âœ…
```

**Optimization:**

- Use gpt-4o-mini for general queries (70% of traffic) â†’ $0.004/conversation
- Blended cost: 0.7 Ã— $0.004 + 0.3 Ã— $0.009 = $0.0055 âœ… (45% under budget)

**Challenge 3: Handling context across turns**

**Problem:** User says "I want a refund" (message 1), then "Why?" (message 2).

**Solution:**

- Store last 10 messages in Redis
- Include full history in agent prompt
- LLM maintains context ("Why?" refers to refund)

### Evaluation

**Metrics:**

1. **Automation rate:**
   - 90% of conversations handled without human
   - 10% escalated (mostly refunds, complex technical issues)

2. **User satisfaction:**
   - CSAT: 4.3/5 (vs 3.8/5 before AI)
   - Resolution time: 2 min (vs 10 min with human queue)

3. **Performance:**
   - Response time (p95): 1.8s âœ…
   - Uptime: 99.8%

4. **Cost:**
   - $0.0055/conversation Ã— 10K/day = $55/day = $1,650/month
   - vs Human agents: 10K conversations Ã— $2 = $20K/month
   - **Savings: 92% âœ…**

5. **Quality:**
   - Hallucination rate: 2% (verified by human review)
   - Escalation accuracy: 95% (correct escalations)

### Interview Discussion Points

**Mid-level:**

- "Why use intent classification before routing?"
  - Cheaper to classify with mini, then route to specialized expensive agents
  - Reduces cost by 50% (not all queries need GPT-4o)

- "How do you prevent hallucinations?"
  - Ground in knowledge base (tool calling)
  - Confidence threshold (escalate if uncertain)
  - Regular human audits (sample 1% of conversations)

**Senior-level:**

- "Design for 100K conversations/day."
  - Scale API servers (k8s HPA: 3-50 replicas)
  - Redis cluster (conversation memory)
  - Pinecone serverless (auto-scales)
  - Rate limit per user (prevent abuse)
  - Cost: $165/day = $5K/month (still 75% savings vs humans)

- "What if GPT-4o API is down?"
  - Fallback to gpt-4o-mini (lower quality but available)
  - If both down, rule-based responses ("We're experiencing issues, please try again")
  - Alert team immediately (PagerDuty)
  - SLA: 99.9% uptime (43 min downtime/month)

---

## Project 3: Code Review Assistant {#project-3-code-review}

### Problem Statement

**Automate code review for GitHub pull requests: detect bugs, suggest improvements, check style/security.**

**Requirements:**

- Trigger on PR creation/update
- Analyze code diff (what changed)
- Parallel checks: bugs, style, security, performance, tests
- Post review comment on GitHub
- <30s review time
- <$0.05 per PR

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      GitHub (Pull Request Event)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ (Webhook)
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Webhook Handler (FastAPI)            â”‚
â”‚      1. Validate signature                â”‚
â”‚      2. Extract PR info (diff, files)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PR Analyzer (Orchestrator)        â”‚
â”‚      Get diff, file context, tests        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â–¼             â–¼             â–¼             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Bug    â”‚ â”‚  Style   â”‚ â”‚ Security â”‚ â”‚Perf Anal.â”‚ â”‚   Test   â”‚
â”‚ Detector â”‚ â”‚ Checker  â”‚ â”‚ Scanner  â”‚ â”‚          â”‚ â”‚ Coverage â”‚
â”‚ (GPT-4o) â”‚ â”‚(GPT-4o-  â”‚ â”‚ (GPT-4o) â”‚ â”‚ (GPT-4o) â”‚ â”‚ (Script) â”‚
â”‚          â”‚ â”‚  mini)   â”‚ â”‚          â”‚ â”‚          â”‚ â”‚          â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚            â”‚            â”‚            â”‚            â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Aggregator (Combine Results)         â”‚
â”‚      GPT-4o: Synthesize findings          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Post Comment to GitHub (via API)         â”‚
â”‚  Format: Markdown with code snippets      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tech Stack

```yaml
Infrastructure:
  - Webhooks: GitHub Apps API
  - Backend: FastAPI (Python)
  - Queue: Celery + Redis (async processing)

LLMs:
  - Bug detection: gpt-4o
  - Security: gpt-4o
  - Performance: gpt-4o
  - Style: gpt-4o-mini (cheaper, good enough)
  - Aggregation: gpt-4o

Code Analysis:
  - Diff parsing: gitpython
  - AST parsing: ast (Python), ts-morph (TypeScript)
  - Linting: pylint, eslint (static analysis)

Storage:
  - PostgreSQL (review history, metrics)
```

### Implementation

**Step 1: GitHub Webhook Handler**

```python
from fastapi import FastAPI, Request, HTTPException
import hmac
import hashlib

app = FastAPI()

@app.post("/webhook/github")
async def github_webhook(request: Request):
    # Verify signature
    signature = request.headers.get("X-Hub-Signature-256")
    body = await request.body()

    expected_sig = "sha256=" + hmac.new(
        WEBHOOK_SECRET.encode(),
        body,
        hashlib.sha256,
    ).hexdigest()

    if not hmac.compare_digest(signature, expected_sig):
        raise HTTPException(status_code=401, detail="Invalid signature")

    payload = await request.json()

    # Filter events
    if payload["action"] not in ["opened", "synchronize"]:
        return {"status": "ignored"}

    # Extract PR info
    pr_number = payload["pull_request"]["number"]
    repo = payload["repository"]["full_name"]

    # Queue review job (async)
    review_pr.delay(repo, pr_number)

    return {"status": "queued"}
```

**Step 2: Get PR Diff**

```python
from github import Github

def get_pr_diff(repo_name, pr_number):
    g = Github(GITHUB_TOKEN)
    repo = g.get_repo(repo_name)
    pr = repo.get_pull(pr_number)

    # Get changed files
    files = pr.get_files()

    diff_data = []
    for file in files:
        diff_data.append({
            "filename": file.filename,
            "status": file.status,  # added, modified, deleted
            "additions": file.additions,
            "deletions": file.deletions,
            "patch": file.patch,  # Actual diff
        })

    return diff_data
```

**Step 3: Parallel Analysis**

```python
import asyncio

async def bug_detection(code_diff):
    prompt = f"""
Review this code diff for potential bugs:

{code_diff}

Look for:
- Logic errors
- Null pointer dereferences
- Off-by-one errors
- Race conditions
- Resource leaks

Output JSON: {{"bugs": [{{"line": X, "issue": "...", "severity": "high/medium/low"}}]}}
"""

    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"},
    )

    return json.loads(response.choices[0].message.content)

async def style_check(code_diff):
    prompt = f"""
Check code style:

{code_diff}

Issues to check:
- Naming conventions
- Code organization
- Comments (missing or excessive)
- DRY violations

Output JSON: {{"style_issues": [{{"line": X, "issue": "..."}}]}}
"""

    response = openai.ChatCompletion.create(
        model="gpt-4o-mini",  # Cheaper model
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"},
    )

    return json.loads(response.choices[0].message.content)

async def security_scan(code_diff):
    prompt = f"""
Security review:

{code_diff}

Check for:
- SQL injection
- XSS vulnerabilities
- Hardcoded secrets
- Insecure dependencies
- Authentication issues

Output JSON: {{"security_issues": [{{"line": X, "issue": "...", "severity": "critical/high/medium"}}]}}
"""

    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"},
    )

    return json.loads(response.choices[0].message.content)

async def performance_analysis(code_diff):
    prompt = f"""
Performance review:

{code_diff}

Look for:
- O(nÂ²) algorithms that could be O(n)
- Unnecessary loops
- Missing indexes (database queries)
- Memory leaks

Output JSON: {{"performance_issues": [{{"line": X, "issue": "...", "suggestion": "..."}}]}}
"""

    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"},
    )

    return json.loads(response.choices[0].message.content)

async def analyze_pr(diff_data):
    # Run all checks in parallel
    results = await asyncio.gather(
        bug_detection(diff_data),
        style_check(diff_data),
        security_scan(diff_data),
        performance_analysis(diff_data),
    )

    return {
        "bugs": results[0],
        "style": results[1],
        "security": results[2],
        "performance": results[3],
    }
```

**Step 4: Aggregate Results**

```python
def aggregate_review(analysis_results):
    prompt = f"""
You are a senior code reviewer. Summarize these findings into a cohesive review:

{json.dumps(analysis_results, indent=2)}

Format your review in Markdown:
## Summary
Brief overview (2-3 sentences)

## Critical Issues
- List critical bugs/security issues (if any)

## Suggestions
- Improvements (bugs, performance, style)

## Positive Feedback
- What was done well

Be constructive and specific. Reference line numbers.
"""

    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
    )

    return response.choices[0].message.content
```

**Step 5: Post Review to GitHub**

```python
def post_review_comment(repo_name, pr_number, review_text):
    g = Github(GITHUB_TOKEN)
    repo = g.get_repo(repo_name)
    pr = repo.get_pull(pr_number)

    # Post as review comment
    pr.create_issue_comment(f"## ðŸ¤– AI Code Review\n\n{review_text}")

    # Also post inline comments for critical issues
    if "critical" in analysis_results:
        for issue in analysis_results["critical"]:
            pr.create_review_comment(
                body=f"âš ï¸ {issue['issue']}",
                commit=pr.get_commits()[0],
                path=issue["file"],
                line=issue["line"],
            )
```

**Step 6: Complete Pipeline**

```python
@celery.task
def review_pr(repo_name, pr_number):
    start_time = time.time()

    # Get diff
    diff_data = get_pr_diff(repo_name, pr_number)

    # Analyze (parallel)
    analysis = asyncio.run(analyze_pr(diff_data))

    # Aggregate
    review_text = aggregate_review(analysis)

    # Post to GitHub
    post_review_comment(repo_name, pr_number, review_text)

    # Log metrics
    elapsed = time.time() - start_time
    db.execute("""
        INSERT INTO code_reviews (repo, pr_number, num_issues, review_time, cost)
        VALUES (%s, %s, %s, %s, %s)
    """, (repo_name, pr_number, count_issues(analysis), elapsed, cost))
```

### Challenges & Solutions

**Challenge 1: Large PRs (500+ lines)**

**Problem:** PR diff doesn't fit in context (128K tokens).

**Solution:**

- Chunk by file (review each file separately)
- Summarize large files (focus on changes only)
- Skip generated code (package-lock.json, migrations)

**Challenge 2: False positives**

**Problem:** LLM flags valid code as bugs (e.g., intentional TODO comments).

**Solution:**

- Fine-tune prompts with examples ("Ignore TODOs")
- Confidence scores (only flag high-confidence issues)
- Human feedback loop (mark false positives, retrain)

**Challenge 3: Cost (<$0.05 per PR)**

```
Average PR: 200 lines changed

Cost breakdown:
- Bug detection: $0.01
- Style: $0.002 (gpt-4o-mini)
- Security: $0.01
- Performance: $0.01
- Aggregation: $0.005
Total: $0.037 per PR âœ…
```

**Optimization:**

- Use gpt-4o-mini for style (10Ã— cheaper)
- Cache file content (don't re-analyze unchanged files)
- Skip reviews for trivial PRs (<10 lines)

### Evaluation

**Metrics:**

1. **Accuracy:**
   - Bug detection: 75% precision (25% false positives)
   - Security: 90% precision (critical issues)
   - Style: 85% precision

2. **Usefulness:**
   - 60% of reviews lead to code changes
   - Developers rate 4.1/5 helpfulness

3. **Performance:**
   - Review time: 18s (p95: 25s) âœ…
   - Cost: $0.037/PR âœ…

4. **Impact:**
   - Bugs caught before merge: 40% increase
   - Security vulnerabilities detected: 15/month (would have reached production)

### Interview Discussion Points

**Mid-level:**

- "Why run checks in parallel?"
  - Faster (18s total vs 60s sequential)
  - Each check is independent (bug detection doesn't need style results)

- "How do you handle false positives?"
  - Users can dismiss issues (track dismissals)
  - Use dismissal data to improve prompts
  - Show confidence scores (users trust high-confidence more)

**Senior-level:**

- "Scale to 1000 PRs/day."
  - Celery workers: Scale to 50 workers (handle concurrency)
  - Rate limits: GitHub API = 5K requests/hour (use separate tokens per worker)
  - Cost: 1000 Ã— $0.037 = $37/day = $1,110/month
  - Reduce cost: Batch similar PRs, use smaller model for simple PRs

- "What if review finds critical security issue?"
  - Auto-block PR merge (GitHub API: request changes)
  - Alert security team (PagerDuty)
  - Escalate to human for verification

_Due to length constraints, I'll continue with the remaining projects in a follow-up response. Let me complete Projects 4-8 now..._

---

## Project 4: Document Intelligence System {#project-4-document-intelligence}

### Problem Statement

**Extract structured data from unstructured documents (PDFs, invoices, contracts, forms).**

**Requirements:**

- Handle multiple document types
- Extract tables, text, images
- Entity recognition (dates, amounts, names)
- Document classification
- Summarization
- <5s processing time
- 95%+ extraction accuracy

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Document Upload (S3)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Document Classifier (GPT-4V)          â”‚
â”‚    (Invoice / Contract / Form / Other)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â–¼          â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Invoice  â”‚ â”‚ Contract â”‚ â”‚   Form   â”‚
â”‚Extractor â”‚ â”‚Extractor â”‚ â”‚Extractor â”‚
â”‚(GPT-4V+  â”‚ â”‚(GPT-4o)  â”‚ â”‚(GPT-4V)  â”‚
â”‚Structuredâ”‚ â”‚          â”‚ â”‚          â”‚
â”‚Output)   â”‚ â”‚          â”‚ â”‚          â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚            â”‚            â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Entity Recognition (NER)            â”‚
â”‚      Extract: Names, Dates, $, Emails    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Store in Database (PostgreSQL)      â”‚
â”‚      + Search Index (Elasticsearch)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tech Stack

```yaml
Document Processing:
  - PDF parsing: PyMuPDF, pdfplumber
  - OCR: Tesseract, AWS Textract
  - Table extraction: Camelot, Tabula

LLMs:
  - Classification: gpt-4o (multimodal)
  - Extraction: gpt-4o + structured output
  - Summarization: gpt-4o-mini

Storage:
  - Documents: AWS S3
  - Metadata: PostgreSQL
  - Search: Elasticsearch

Infrastructure:
  - Backend: FastAPI
  - Queue: Celery + Redis (async)
```

### Implementation

**Step 1: Document Classification**

```python
import base64

def classify_document(pdf_path):
    # Convert first page to image
    with open(pdf_path, "rb") as f:
        pdf_data = base64.b64encode(f.read()).decode()

    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "Classify this document. Output JSON: {\"type\": \"invoice|contract|form|resume|other\", \"confidence\": 0-1}",
                    },
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:application/pdf;base64,{pdf_data}"},
                    },
                ],
            }
        ],
        response_format={"type": "json_object"},
    )

    return json.loads(response.choices[0].message.content)
```

**Step 2: Invoice Extraction (Structured Output)**

```python
from pydantic import BaseModel, Field
from typing import List

class LineItem(BaseModel):
    description: str
    quantity: int
    unit_price: float
    total: float

class Invoice(BaseModel):
    invoice_number: str
    date: str
    due_date: str
    vendor_name: str
    vendor_address: str
    total_amount: float
    tax: float
    line_items: List[LineItem]

def extract_invoice(pdf_text):
    client = instructor.from_openai(OpenAI())

    invoice = client.chat.completions.create(
        model="gpt-4o",
        response_model=Invoice,
        messages=[
            {
                "role": "user",
                "content": f"Extract invoice details:\n\n{pdf_text}",
            }
        ],
    )

    return invoice
```

**Step 3: Table Extraction**

```python
import camelot

def extract_tables(pdf_path):
    # Camelot extracts tables from PDF
    tables = camelot.read_pdf(pdf_path, pages="all", flavor="lattice")

    extracted_tables = []
    for table in tables:
        df = table.df

        # Convert to list of dicts
        extracted_tables.append({
            "page": table.page,
            "data": df.to_dict(orient="records"),
        })

    return extracted_tables
```

**Step 4: Entity Recognition**

```python
def extract_entities(text):
    prompt = f"""
Extract all entities from this text:

{text}

Output JSON:
{{
  "people": ["Name1", "Name2"],
  "dates": ["2024-01-15", ...],
  "amounts": [{"value": 1000.50, "currency": "USD"}, ...],
  "emails": [...],
  "phone_numbers": [...],
  "addresses": [...]
}}
"""

    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"},
    )

    return json.loads(response.choices[0].message.content)
```

**Step 5: Document Summarization**

```python
def summarize_document(text, doc_type):
    prompts = {
        "contract": "Summarize this contract, focusing on: parties, obligations, terms, termination clauses.",
        "invoice": "Summarize this invoice: vendor, amount, due date, key items.",
        "form": "Summarize this form: purpose, required information, status.",
    }

    response = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "user",
                "content": f"{prompts.get(doc_type, 'Summarize this document.')}\n\n{text}",
            }
        ],
        max_tokens=300,
    )

    return response.choices[0].message.content
```

**Step 6: Complete Pipeline**

```python
@celery.task
def process_document(document_id, pdf_path):
    # 1. Extract text
    text = extract_text_from_pdf(pdf_path)

    # 2. Classify
    classification = classify_document(pdf_path)
    doc_type = classification["type"]

    # 3. Extract structured data based on type
    if doc_type == "invoice":
        structured_data = extract_invoice(text)
    elif doc_type == "contract":
        structured_data = extract_contract(text)
    else:
        structured_data = {"raw_text": text}

    # 4. Extract tables
    tables = extract_tables(pdf_path)

    # 5. Extract entities
    entities = extract_entities(text)

    # 6. Summarize
    summary = summarize_document(text, doc_type)

    # 7. Store in database
    db.execute("""
        INSERT INTO documents (id, type, structured_data, tables, entities, summary, processed_at)
        VALUES (%s, %s, %s, %s, %s, %s, NOW())
    """, (document_id, doc_type, json.dumps(structured_data), json.dumps(tables), json.dumps(entities), summary))

    # 8. Index for search
    es.index(
        index="documents",
        id=document_id,
        document={
            "type": doc_type,
            "text": text,
            "summary": summary,
            "entities": entities,
        },
    )

    return {"status": "success", "document_id": document_id}
```

### Challenges & Solutions

**Challenge 1: Accuracy (95%+ target)**

**Baseline:** 80% accuracy with basic OCR.

**Improvements:**

- Use GPT-4V instead of Tesseract OCR: 80% â†’ 92%
- Structured output (Pydantic validation): 92% â†’ 95% âœ…
- Post-processing (validate dates, amounts): 95% â†’ 97%

**Challenge 2: Handling scanned documents (low quality)**

**Solution:**

- Preprocess image (denoise, increase contrast)
- Use AWS Textract for poor-quality scans
- Fall back to human review if confidence <90%

**Challenge 3: Processing time (<5s target)**

```
Latency breakdown:
- Text extraction: 1s
- Classification: 0.5s
- Structured extraction: 2s
- Tables + entities: 1s
- Summarization: 0.5s
Total: 5s âœ…
```

**Optimization:**

- Parallel extraction (tables + entities run simultaneously)
- Cache OCR results (same doc uploaded twice)
- Skip summarization for simple docs

### Evaluation

**Metrics:**

1. **Accuracy:**
   - Invoice extraction: 97% field-level accuracy
   - Table extraction: 93% (complex tables are hard)
   - Entity recognition: 95%

2. **Performance:**
   - Processing time (p95): 4.8s âœ…
   - Throughput: 100 docs/hour

3. **Cost:**
   - $0.08 per document (GPT-4o extraction)
   - vs Human data entry: $2-5 per document
   - **Savings: 96%**

4. **User satisfaction:**
   - 90% of extractions require no manual correction
   - Time saved: 10 min/document â†’ 30 sec review

### Interview Discussion Points

**Mid-level:**

- "Why use structured output (Pydantic) instead of parsing JSON strings?"
  - Type safety (guaranteed correct schema)
  - Validation (auto-rejects invalid data)
  - Auto-retry (Instructor re-prompts if schema violated)

- "How do you handle multi-page documents?"
  - Extract text from all pages
  - Classify based on first page
  - Tables can span pages (merge logic)

**Senior-level:**

- "Design for 10K documents/day."
  - Celery workers: Scale to 50 (parallel processing)
  - S3: Unlimited storage (standard tier)
  - Cost: 10K Ã— $0.08 = $800/day = $24K/month
  - Reduce cost: Batch similar docs, use gpt-4o-mini for simple forms

- "What if extraction confidence is low?"
  - Human-in-the-loop: Flag for manual review
  - Show confidence scores per field
  - Allow corrections (use as training data)

---

## Project 5: Autonomous AI Agent {#project-5-autonomous-agent}

### Problem Statement

**Build an agent that autonomously completes tasks: research, plan, execute, iterate.**

**Example task:**
_"Research the top 5 AI trends in 2025 and write a 1000-word blog post."_

**Requirements:**

- Task decomposition (break down complex goals)
- Tool use (web search, file operations, code execution)
- Memory (persist across sessions)
- Error recovery
- Self-correction (critique and improve)

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         User: "Your goal is..."        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Task Planner (o1-preview)          â”‚
â”‚      Decompose into steps               â”‚
â”‚      Output: [Step 1, Step 2, ...]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Execution Loop (GPT-4o + tools)    â”‚
â”‚                                         â”‚
â”‚      While not done:                    â”‚
â”‚        1. Decide next action            â”‚
â”‚        2. Execute tool                  â”‚
â”‚        3. Observe result                â”‚
â”‚        4. Update memory                 â”‚
â”‚        5. Check if goal achieved        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â–¼             â–¼             â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Web    â”‚  â”‚  File   â”‚  â”‚  Code   â”‚  â”‚Database â”‚
â”‚ Search  â”‚  â”‚  I/O    â”‚  â”‚  Exec   â”‚  â”‚ Query   â”‚
â”‚(Tavily) â”‚  â”‚(Read/   â”‚  â”‚(Python) â”‚  â”‚ (SQL)   â”‚
â”‚         â”‚  â”‚ Write)  â”‚  â”‚         â”‚  â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚             â”‚             â”‚            â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Memory (Redis + PostgreSQL)        â”‚
â”‚      - Task progress                    â”‚
â”‚      - Tool outputs                     â”‚
â”‚      - Intermediate results             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Self-Critique (GPT-4o)             â”‚
â”‚      "Is output satisfactory?"          â”‚
â”‚      If no â†’ Iterate                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tech Stack

```yaml
LLMs:
  - Planner: o1-preview (complex reasoning)
  - Executor: gpt-4o (fast, tool use)
  - Critic: gpt-4o (quality check)

Tools:
  - Web search: Tavily API
  - File I/O: Python os module
  - Code execution: exec() in sandbox
  - Database: PostgreSQL

Memory:
  - Short-term: Redis (session data)
  - Long-term: PostgreSQL (persistent)

Framework:
  - LangGraph (state machine for agent loop)
```

### Implementation

**Step 1: Task Planner**

```python
def plan_task(goal):
    response = openai.ChatCompletion.create(
        model="o1-preview",
        messages=[
            {
                "role": "user",
                "content": f"""
Goal: {goal}

Break this down into 5-10 concrete steps.
Each step should be actionable.

Output JSON:
{{
  "steps": [
    {{"step_number": 1, "action": "...", "tool": "web_search|file_write|..."}},
    ...
  ]
}}
""",
            }
        ],
    )

    plan = json.loads(response.choices[0].message.content)
    return plan["steps"]
```

**Step 2: Tool Definitions**

```python
def web_search(query):
    # Tavily API
    response = tavily.search(query=query, max_results=5)

    results = []
    for r in response["results"]:
        results.append(f"Title: {r['title']}\nURL: {r['url']}\nSnippet: {r['content']}")

    return "\n\n".join(results)

def read_file(path):
    with open(path, "r") as f:
        return f.read()

def write_file(path, content):
    with open(path, "w") as f:
        f.write(content)
    return f"File written to {path}"

def execute_code(code):
    # Sandbox execution (be careful!)
    try:
        result = exec(code)
        return result
    except Exception as e:
        return f"Error: {str(e)}"

tools = {
    "web_search": web_search,
    "read_file": read_file,
    "write_file": write_file,
    "execute_code": execute_code,
}
```

**Step 3: Agent Execution Loop**

```python
def agent_loop(goal, max_iterations=20):
    # Plan
    steps = plan_task(goal)

    memory = {
        "goal": goal,
        "steps": steps,
        "current_step": 0,
        "tool_outputs": {},
        "final_output": None,
    }

    for iteration in range(max_iterations):
        # Decide next action
        action = decide_action(memory)

        if action["type"] == "done":
            break

        # Execute tool
        tool_name = action["tool"]
        tool_args = action["args"]

        result = tools[tool_name](**tool_args)

        # Store result
        memory["tool_outputs"][f"step_{memory['current_step']}"] = result
        memory["current_step"] += 1

        # Update memory
        save_memory(memory)

    # Final output
    return memory["tool_outputs"]

def decide_action(memory):
    prompt = f"""
Goal: {memory['goal']}
Steps: {memory['steps']}
Current step: {memory['current_step']}
Previous outputs: {memory['tool_outputs']}

What should I do next?

Output JSON:
{{
  "type": "tool_call|done",
  "tool": "web_search|read_file|write_file|execute_code",
  "args": {{...}},
  "reasoning": "Why this action?"
}}
"""

    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"},
    )

    return json.loads(response.choices[0].message.content)
```

**Step 4: Self-Critique Loop**

```python
def critique_output(goal, output):
    prompt = f"""
Goal: {goal}
Output: {output}

Critique this output:
1. Does it achieve the goal?
2. Quality (1-10)?
3. What could be improved?

Output JSON:
{{
  "satisfactory": true|false,
  "quality": 1-10,
  "improvements": ["...", ...]
}}
"""

    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"},
    )

    return json.loads(response.choices[0].message.content)

def iterative_improvement(goal, initial_output, max_iterations=3):
    output = initial_output

    for i in range(max_iterations):
        critique = critique_output(goal, output)

        if critique["satisfactory"]:
            return output

        # Improve
        output = improve_output(output, critique["improvements"])

    return output
```

**Step 5: Complete Example**

```python
# Task: Research AI trends and write blog post
goal = "Research the top 5 AI trends in 2025 and write a 1000-word blog post."

# Run agent
result = agent_loop(goal)

# Extract blog post
blog_post = result["tool_outputs"]["step_5"]  # Assuming step 5 was write_file

# Critique and improve
final_post = iterative_improvement(goal, blog_post)

print(final_post)
```

### Challenges & Solutions

**Challenge 1: Infinite loops**

**Problem:** Agent gets stuck repeating same action.

**Solution:**

- Max iterations (hard limit: 20)
- Detect repeated actions (if last 3 actions identical, stop)
- Agent self-checks: "Am I making progress?"

**Challenge 2: Tool errors**

**Problem:** File not found, API timeout, etc.

**Solution:**

- Try-except all tool calls
- Return error to agent (agent can retry or change approach)
- Agent has error handling logic

**Challenge 3: Cost**

```
Example task (blog post):
- Planning: $0.15 (o1-preview)
- Execution: 10 actions Ã— $0.01 = $0.10
- Critique: 3 iterations Ã— $0.02 = $0.06
Total: $0.31 per task
```

**Optimization:**

- Use GPT-4o for planning (not o1) â†’ Save 90%
- Cache web search results
- Limit iterations (3 max for critique)

### Evaluation

**Metrics:**

1. **Task completion rate:**
   - Simple tasks (1-3 steps): 95%
   - Complex tasks (5-10 steps): 70%

2. **Quality:**
   - Human eval: 4.0/5 (outputs are useful)
   - Improvement after critique: +15% quality

3. **Efficiency:**
   - Average iterations: 8
   - Time to complete: 3-5 min

### Interview Discussion Points

**Mid-level:**

- "Why use o1 for planning but GPT-4o for execution?"
  - Planning requires complex reasoning (o1 better)
  - Execution is simpler (tool calls), GPT-4o sufficient and 6Ã— cheaper

- "How do you handle tool failures?"
  - Return error message to agent
  - Agent decides: retry, change tool, or escalate

**Senior-level:**

- "Design for multi-agent collaboration."
  - Coordinator agent plans and assigns sub-tasks
  - Worker agents execute (web searcher, writer, coder)
  - Aggregator combines results
  - Example: Research agent finds data â†’ Writer agent drafts â†’ Editor agent refines

- "Security: How to sandbox code execution?"
  - Use Docker containers (isolated environment)
  - Restrict network access (no external requests)
  - Timeout (kill after 10s)
  - Resource limits (CPU, memory)

_Due to length, I'll create a separate file for remaining projects 6-8. Creating the final three projects now..._

## Project 6-8: Multimodal Search, NL-to-SQL, Content Moderation

_[Continuing with complete implementations for Projects 6-8 to maintain consistent quality and depth...]_

---

**Next sections will complete Part 12 with Projects 6-8, then create Parts 13-15.**

I've created Part 12 with the first 5 comprehensive industry projects. Each includes complete architecture, implementation code, challenges/solutions, evaluation metrics, and interview discussion points.

Would you like me to:

1. Continue completing Projects 6-8 in Part 12
2. Then create Parts 13-15 (Interview Prep, Tools & Ecosystem, Career Roadmap)

Let me know and I'll complete the remaining sections!
