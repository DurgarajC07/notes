# âš–ï¸ OLTP vs OLAP - Transaction vs Analytics Workloads

> Understanding the fundamental differences between OLTP and OLAP systems is crucial for choosing the right database architecture and optimization strategies for your workload.

---

## ğŸ“– 1. Concept Explanation

### What are OLTP and OLAP?

**OLTP (Online Transaction Processing):**

- Handles **day-to-day operational transactions**
- INSERT, UPDATE, DELETE operations
- Serves **live users and applications**
- Row-oriented, normalized data
- Example: E-commerce checkout, bank transfers, booking systems

**OLAP (Online Analytical Processing):**

- Handles **complex analytical queries**
- SELECT with aggregations (SUM, AVG, COUNT)
- Serves **analysts and business intelligence**
- Column-oriented, denormalized data
- Example: Sales reports, data warehouses, business dashboards

---

### Key Differences

```
OLTP Database                    OLAP Database
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Row-oriented storage             Column-oriented storage
Normalized (3NF)                 Denormalized (star/snowflake)
Small, frequent transactions     Large, complex queries
High write volume                High read volume
Sub-second response time         Seconds to minutes
Current data                     Historical data
Hundreds of concurrent users     Dozens of concurrent queries
GBs to TBs                       TBs to PBs
```

---

### Visual Comparison

```
OLTP: E-Commerce Order
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INSERT INTO orders VALUES (...)     â”‚ â† Single row insert (10ms)
â”‚ UPDATE inventory SET qty = qty - 1  â”‚ â† Single row update (5ms)
â”‚ INSERT INTO payments VALUES (...)   â”‚ â† Single row insert (8ms)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Result: 3 operations, 23ms total âš¡

OLAP: Monthly Sales Report
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SELECT                                â”‚
â”‚   product_category,                   â”‚
â”‚   DATE_TRUNC('month', order_date),    â”‚
â”‚   SUM(total_amount),                  â”‚
â”‚   AVG(order_value),                   â”‚
â”‚   COUNT(DISTINCT customer_id)         â”‚
â”‚ FROM orders                           â”‚
â”‚ WHERE order_date >= '2024-01-01'      â”‚
â”‚ GROUP BY 1, 2                         â”‚
â”‚ ORDER BY 2 DESC;                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Result: Scans 10M rows, 30 seconds ğŸ¢
```

---

## ğŸ§  2. Why It Matters in Real Systems

### The Problem: Using OLTP for OLAP (Anti-Pattern)

```sql
-- âŒ Running analytics on production OLTP database
-- Business analyst runs this query at 2pm (peak traffic)

SELECT
    products.category,
    DATE_TRUNC('month', orders.created_at) AS month,
    SUM(order_items.quantity * order_items.price) AS revenue,
    COUNT(DISTINCT orders.customer_id) AS unique_customers
FROM orders
JOIN order_items ON orders.id = order_items.order_id
JOIN products ON order_items.product_id = products.id
WHERE orders.created_at >= '2023-01-01'
GROUP BY 1, 2
ORDER BY 2 DESC;

-- Query scans 50M rows (1 year of data)
-- Takes 5 minutes
-- Locks tables, blocks writes
-- Production site slows to a crawl ğŸ’¥
-- Customers abandon shopping carts
-- Revenue lost!
```

**Impact:**

- ğŸ”¥ Production site goes down
- ğŸ”¥ Customer complaints spike
- ğŸ”¥ Lost revenue
- ğŸ”¥ Database CPU at 100%
- ğŸ”¥ Connection pool exhausted

---

### The Solution: Separate OLTP and OLAP

```
Architecture:

OLTP Database (PostgreSQL/MySQL)
â”œâ”€ Handles: Orders, payments, inventory updates
â”œâ”€ Optimized: Row storage, indexes, transactions
â”œâ”€ Users: Live customers (100,000 concurrent)
â””â”€ Response: <100ms per request

        â†“ ETL Pipeline (nightly)

OLAP Database (Snowflake/BigQuery/Redshift)
â”œâ”€ Handles: Analytics, reports, dashboards
â”œâ”€ Optimized: Column storage, compression, partitioning
â”œâ”€ Users: Analysts, BI tools (50 concurrent)
â””â”€ Response: 10s - 5min per query

Result:
âœ… Production unaffected by analytics
âœ… Analytics gets optimized storage format
âœ… Both systems perform optimally
```

---

### Real-World Scale

| System Type | Example Company  | Daily Ops    | Storage | Latency |
| ----------- | ---------------- | ------------ | ------- | ------- |
| **OLTP**    | Amazon.com       | 50M orders   | 10 TB   | <50ms   |
| **OLAP**    | Amazon Analytics | 1000 queries | 500 TB  | 30s-5m  |
| **OLTP**    | Uber             | 15M rides    | 5 TB    | <100ms  |
| **OLAP**    | Uber Analytics   | 500 queries  | 200 TB  | 1-10m   |

---

## âš™ï¸ 3. Internal Working

### OLTP Storage: Row-Oriented

**How data is stored:**

```
Table: orders (row-oriented)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[Row 1: id=1, user_id=100, total=99.50, status='paid']
[Row 2: id=2, user_id=101, total=45.00, status='pending']
[Row 3: id=3, user_id=102, total=150.00, status='paid']

Disk layout:
Page 1: [Row1][Row2][Row3]...
Page 2: [Row101][Row102][Row103]...
```

**Query: Fetch single order**

```sql
SELECT * FROM orders WHERE id = 1;

Execution:
1. Index lookup: id=1 â†’ Page 1, Offset 0
2. Read single page (8KB)
3. Return 1 row

Result: 1 I/O operation, <1ms âš¡
```

**Why it's fast for OLTP:**

- âœ… Entire row in one disk read
- âœ… Perfect for INSERT/UPDATE (single write)
- âœ… B-Tree index for point lookups

---

### OLAP Storage: Column-Oriented

**How data is stored:**

```
Table: orders (column-oriented)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Column: id        â†’ [1, 2, 3, 4, 5, ...]
Column: user_id   â†’ [100, 101, 102, 103, 104, ...]
Column: total     â†’ [99.50, 45.00, 150.00, 30.00, ...]
Column: status    â†’ ['paid', 'pending', 'paid', 'paid', ...]

Disk layout:
File 1 (id column):        [1][2][3][4]...
File 2 (user_id column):   [100][101][102][103]...
File 3 (total column):     [99.50][45.00][150.00]...
File 4 (status column):    ['paid']['pending']['paid']...
```

**Query: Aggregate sales**

```sql
SELECT SUM(total) FROM orders WHERE status = 'paid';

Execution:
1. Read status column file (compressed: 100MB â†’ 5MB)
2. Filter rows where status='paid' (bitmap index)
3. Read total column file for matching rows only
4. SUM(total)

Result: 2 columns read (not all 20 columns!), 500ms âœ…
```

**Why it's fast for OLAP:**

- âœ… Only reads needed columns (skip 18 other columns)
- âœ… Excellent compression (repeating values)
- âœ… Vectorized execution (process 1000s of values at once)
- âœ… Parallel scan (multiple cores)

---

### OLTP vs OLAP Indexes

**OLTP Indexes (B-Tree):**

```sql
-- B-Tree index on orders(customer_id)
CREATE INDEX idx_customer ON orders(customer_id);

Query:
SELECT * FROM orders WHERE customer_id = 12345;

Index scan:
1. Root node   â†’ pointer to branch
2. Branch node â†’ pointer to leaf
3. Leaf node   â†’ pointer to row (page 42, offset 10)
4. Fetch row from page 42

Result: 4 I/O operations (index) + 1 I/O (row fetch) = <5ms
```

**OLAP Indexes (Bitmap, Partitioning):**

```sql
-- Bitmap index on orders(status)
CREATE BITMAP INDEX idx_status ON orders(status);

Bitmap:
status='paid':     [1,0,1,1,0,1,0,...]  â† 1 bit per row
status='pending':  [0,1,0,0,1,0,0,...]

Query:
SELECT SUM(total) FROM orders WHERE status = 'paid';

Bitmap scan:
1. Read bitmap (tiny: 1 million rows = 125KB)
2. Filter rows in parallel
3. Read only matching row segments
4. Aggregate

Result: Scans 10M rows in 2 seconds âœ…
```

---

## âœ… 4. Best Practices

### 1. Separate OLTP and OLAP Workloads

```
âŒ Bad: Mixed workload on single database
Production DB (PostgreSQL)
â”œâ”€ Customer transactions (OLTP)
â””â”€ Analyst queries (OLAP) â† ğŸ’¥ Kills production!

âœ… Good: Dedicated systems
Production DB (PostgreSQL/MySQL)
â”œâ”€ Customer transactions (OLTP)
â””â”€ Optimized for writes, indexed, normalized

        â†“ ETL/CDC Pipeline

Analytics DB (Snowflake/BigQuery)
â”œâ”€ Business reports (OLAP)
â””â”€ Optimized for reads, columnar, denormalized
```

---

### 2. Use Read Replicas for OLTP Reporting

```sql
-- âœ… Route read-only queries to replica
# Application config
DATABASE_URLS = {
    'primary': 'postgresql://primary-db:5432/app',
    'replica': 'postgresql://replica-db:5432/app'
}

# Django ORM example
class Order(models.Model):
    # Writes go to primary
    Order.objects.create(user_id=123, total=99.50)

    # Heavy reads go to replica
    orders = Order.objects.using('replica').filter(
        created_at__gte='2024-01-01'
    ).aggregate(total_sales=Sum('total'))
```

**Benefits:**

- âœ… Offloads read load from primary
- âœ… Primary handles writes without contention
- âœ… Replica can have different indexes for analytics

**Trade-offs:**

- âš ï¸ Replica lag (eventual consistency)
- âš ï¸ Not suitable for real-time reports

---

### 3. Optimize OLTP for Writes

```sql
-- âœ… OLTP best practices
-- 1. Normalized schema (avoid redundancy)
CREATE TABLE orders (
    id SERIAL PRIMARY KEY,
    customer_id INT NOT NULL,
    total DECIMAL(10,2) NOT NULL,
    created_at TIMESTAMP NOT NULL
);

CREATE TABLE order_items (
    id SERIAL PRIMARY KEY,
    order_id INT NOT NULL REFERENCES orders(id),
    product_id INT NOT NULL,
    quantity INT NOT NULL,
    price DECIMAL(10,2) NOT NULL
);

-- 2. Indexes on foreign keys
CREATE INDEX idx_order_items_order_id ON order_items(order_id);

-- 3. Partitioning by time for large tables
CREATE TABLE orders_2024_01 PARTITION OF orders
    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');
```

---

### 4. Optimize OLAP for Reads

```sql
-- âœ… OLAP best practices
-- 1. Denormalized schema (star schema)
CREATE TABLE fact_sales (
    date_key INT,
    product_key INT,
    customer_key INT,
    store_key INT,
    quantity INT,
    revenue DECIMAL(10,2),
    cost DECIMAL(10,2)
) PARTITION BY RANGE (date_key);

-- 2. Columnar storage (PostgreSQL)
CREATE TABLE fact_sales (...) USING columnar;

-- 3. Pre-aggregated tables
CREATE MATERIALIZED VIEW monthly_sales AS
SELECT
    DATE_TRUNC('month', date) AS month,
    product_category,
    SUM(revenue) AS total_revenue
FROM fact_sales
GROUP BY 1, 2;

-- Refresh nightly
REFRESH MATERIALIZED VIEW CONCURRENTLY monthly_sales;
```

---

## âŒ 5. Common Mistakes

### Mistake 1: Running OLAP Queries on OLTP Production Database

```sql
-- âŒ Disaster: Complex analytics on production
SELECT
    c.country,
    p.category,
    DATE_TRUNC('month', o.created_at) AS month,
    SUM(oi.quantity * oi.price) AS revenue,
    COUNT(DISTINCT o.customer_id) AS customers,
    AVG(o.total) AS avg_order_value
FROM orders o
JOIN order_items oi ON o.id = oi.order_id
JOIN products p ON oi.product_id = p.id
JOIN customers c ON o.customer_id = c.id
WHERE o.created_at >= '2022-01-01'  -- 2 years of data!
GROUP BY 1, 2, 3
ORDER BY 4 DESC;

-- Execution time: 10 minutes ğŸ¢
-- CPU: 100% ğŸ”¥
-- Locks: 50+ tables ğŸ”’
-- Production: DOWN ğŸ’¥
```

**Solution:**

```python
# âœ… Move heavy analytics to dedicated OLAP database
# ETL pipeline runs nightly
def etl_pipeline():
    # Extract from OLTP
    orders = oltp_db.query("SELECT * FROM orders WHERE created_at >= yesterday")

    # Transform
    aggregated = aggregate_sales(orders)

    # Load to OLAP
    olap_db.bulk_insert('fact_sales', aggregated)

# Analysts query OLAP database
analytics_query = """
    SELECT country, category, month, SUM(revenue)
    FROM fact_sales
    WHERE month >= '2022-01-01'
    GROUP BY 1, 2, 3
"""
# Runs on OLAP warehouse, doesn't affect production âœ…
```

---

### Mistake 2: Using Normalized Schema for OLAP

```sql
-- âŒ Bad: Normalized schema in data warehouse
SELECT
    SUM(oi.quantity * oi.price)
FROM orders o
JOIN order_items oi ON o.id = oi.order_id
JOIN products p ON oi.product_id = p.id
JOIN categories c ON p.category_id = c.id;

-- 4 table joins on 100M rows = 5 minutes ğŸ¢

-- âœ… Good: Denormalized star schema
SELECT SUM(revenue)
FROM fact_sales
WHERE category = 'Electronics';

-- Single table scan = 5 seconds âš¡
```

---

### Mistake 3: Not Using Partitioning for OLAP

```sql
-- âŒ Bad: Single huge table
CREATE TABLE events (
    id BIGSERIAL PRIMARY KEY,
    user_id INT,
    event_type VARCHAR(50),
    created_at TIMESTAMP
);

-- Query: Last 7 days
SELECT COUNT(*) FROM events
WHERE created_at >= NOW() - INTERVAL '7 days';

-- Scans entire 500GB table! ğŸ’¥

-- âœ… Good: Partitioned by date
CREATE TABLE events (
    id BIGSERIAL,
    user_id INT,
    event_type VARCHAR(50),
    created_at TIMESTAMP
) PARTITION BY RANGE (created_at);

CREATE TABLE events_2024_02 PARTITION OF events
    FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');

-- Query: Last 7 days
SELECT COUNT(*) FROM events
WHERE created_at >= NOW() - INTERVAL '7 days';

-- Only scans 1 partition (5GB) âœ…
```

---

### Mistake 4: No Caching Layer for OLAP

```python
# âŒ Bad: Re-run expensive queries every time
@app.route('/dashboard')
def dashboard():
    # This query takes 30 seconds!
    sales_data = olap_db.query("""
        SELECT * FROM monthly_sales_summary
        WHERE year = 2024
    """)
    return render_template('dashboard.html', data=sales_data)

# Every page load waits 30 seconds ğŸ¢

# âœ… Good: Cache results
@app.route('/dashboard')
@cache.cached(timeout=3600)  # Cache for 1 hour
def dashboard():
    sales_data = olap_db.query("""
        SELECT * FROM monthly_sales_summary
        WHERE year = 2024
    """)
    return render_template('dashboard.html', data=sales_data)

# First load: 30 seconds
# Subsequent loads: <100ms âš¡
```

---

## ğŸ” 6. Security Considerations

### 1. Separate Access Control

```sql
-- âœ… Different permissions for OLTP vs OLAP
-- OLTP: Application users (limited access)
CREATE ROLE app_user;
GRANT SELECT, INSERT, UPDATE ON orders TO app_user;
GRANT SELECT, INSERT, UPDATE ON order_items TO app_user;
-- No DELETE, no DROP, no admin access

-- OLAP: Analysts (read-only, broader access)
CREATE ROLE analyst;
GRANT SELECT ON ALL TABLES IN SCHEMA analytics TO analyst;
-- Read-only, can query anything in analytics schema
```

---

### 2. Data Masking for OLAP

```sql
-- âœ… Mask PII in analytics database
CREATE VIEW fact_sales_masked AS
SELECT
    sale_id,
    date_key,
    product_key,
    CONCAT(SUBSTRING(customer_email, 1, 2), '***@***') AS customer_email_masked,
    revenue,
    cost
FROM fact_sales;

GRANT SELECT ON fact_sales_masked TO analyst;
-- Analysts see masked data, compliance maintained âœ…
```

---

### 3. Query Timeouts

```sql
-- âœ… Set query timeout to prevent runaway queries
-- PostgreSQL
SET statement_timeout = '5min';

-- MySQL
SET max_execution_time = 300000;  -- 5 minutes

-- Prevents accidental full table scans from killing database
```

---

## ğŸš€ 7. Performance Optimization Techniques

### 1. OLTP: Use Connection Pooling

```python
# âœ… Connection pooling for OLTP
from sqlalchemy import create_engine, pool

engine = create_engine(
    'postgresql://localhost/app',
    poolclass=pool.QueuePool,
    pool_size=20,           # 20 persistent connections
    max_overflow=10,        # + 10 overflow connections
    pool_timeout=30,        # Wait 30s for connection
    pool_recycle=3600       # Recycle connections after 1 hour
)

# Handles 1000s of requests/sec with only 30 connections
```

---

### 2. OLAP: Use Columnar Compression

```sql
-- âœ… Enable compression in columnar storage
-- BigQuery (automatic)
CREATE TABLE sales (
    date DATE,
    product_id INT64,
    revenue FLOAT64
)
PARTITION BY date
CLUSTER BY product_id;

-- Compression ratio: 10:1 or better
-- 1TB raw data â†’ 100GB stored âœ…

-- PostgreSQL with cstore_fdw
CREATE FOREIGN TABLE sales (
    date DATE,
    product_id INT,
    revenue DECIMAL(10,2)
)
SERVER cstore_server
OPTIONS (compression 'pglz');
```

---

### 3. OLTP: Index Tuning

```sql
-- âœ… Composite index for common query patterns
-- Query: Find user's recent orders
SELECT * FROM orders
WHERE customer_id = 123
ORDER BY created_at DESC
LIMIT 10;

-- Composite index (customer_id, created_at DESC)
CREATE INDEX idx_customer_recent
ON orders(customer_id, created_at DESC);

-- Index-only scan, no table access needed âš¡
```

---

### 4. OLAP: Pre-Aggregation

```sql
-- âœ… Pre-aggregate common metrics
CREATE MATERIALIZED VIEW daily_sales_summary AS
SELECT
    DATE(created_at) AS date,
    product_category,
    COUNT(*) AS order_count,
    SUM(total) AS total_revenue,
    AVG(total) AS avg_order_value
FROM orders
GROUP BY 1, 2;

-- Refresh nightly
REFRESH MATERIALIZED VIEW CONCURRENTLY daily_sales_summary;

-- Queries hit summary (10K rows) instead of raw orders (100M rows)
-- 1000Ã— faster âœ…
```

---

## ğŸ§ª 8. Examples

### Example 1: E-Commerce System

**OLTP Database (PostgreSQL):**

```sql
-- Normalized schema for transactions
CREATE TABLE orders (
    id SERIAL PRIMARY KEY,
    customer_id INT NOT NULL,
    total DECIMAL(10,2) NOT NULL,
    status VARCHAR(20) NOT NULL,
    created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_orders_customer ON orders(customer_id);
CREATE INDEX idx_orders_created ON orders(created_at);

-- Typical transaction (5ms)
BEGIN;
    INSERT INTO orders (customer_id, total, status)
    VALUES (12345, 99.50, 'pending')
    RETURNING id;

    INSERT INTO order_items (order_id, product_id, quantity, price)
    VALUES (1001, 456, 2, 49.75);

    UPDATE inventory SET quantity = quantity - 2
    WHERE product_id = 456;
COMMIT;
```

**OLAP Data Warehouse (Snowflake):**

```sql
-- Denormalized star schema for analytics
CREATE TABLE fact_sales (
    sale_id INT,
    date_key INT,
    product_key INT,
    customer_key INT,

    -- Denormalized dimensions
    product_name VARCHAR(100),
    product_category VARCHAR(50),
    customer_country VARCHAR(50),
    customer_segment VARCHAR(20),

    -- Metrics
    quantity INT,
    revenue DECIMAL(10,2),
    cost DECIMAL(10,2),
    profit DECIMAL(10,2)
)
CLUSTER BY (date_key);

-- Typical query (30s on 1B rows)
SELECT
    customer_country,
    product_category,
    SUM(revenue) AS total_revenue,
    SUM(profit) AS total_profit
FROM fact_sales
WHERE date_key >= 20240101
GROUP BY 1, 2
ORDER BY 3 DESC
LIMIT 100;
```

---

### Example 2: SaaS Application with Mixed Workload

```python
# âœ… Route queries to appropriate database
class AnalyticsService:
    def __init__(self):
        self.oltp_db = connect_to_postgres()  # Production DB
        self.olap_db = connect_to_snowflake()  # Analytics DB

    def create_order(self, order_data):
        """OLTP: Write to production database"""
        with self.oltp_db.transaction():
            order_id = self.oltp_db.insert('orders', order_data)
            return order_id

    def get_customer_orders(self, customer_id):
        """OLTP: Read recent orders (real-time)"""
        return self.oltp_db.query("""
            SELECT * FROM orders
            WHERE customer_id = %s
            ORDER BY created_at DESC
            LIMIT 20
        """, [customer_id])

    def get_monthly_revenue_report(self, year, month):
        """OLAP: Complex analytics query"""
        return self.olap_db.query("""
            SELECT
                product_category,
                SUM(revenue) AS total_revenue,
                COUNT(DISTINCT customer_id) AS unique_customers,
                AVG(order_value) AS avg_order_value
            FROM fact_sales
            WHERE year = %s AND month = %s
            GROUP BY 1
            ORDER BY 2 DESC
        """, [year, month])
```

---

## ğŸ—ï¸ 9. Real-World Use Cases

### Use Case 1: Netflix - Video Streaming Platform

**OLTP: User Interactions**

- Database: Cassandra (distributed writes)
- Workload: User plays video, updates watch history, likes content
- Scale: 100M+ users, 1M requests/sec
- Latency: <50ms

**OLAP: Content Recommendations**

- Database: Spark + S3 Data Lake
- Workload: Analyze viewing patterns, train ML models
- Scale: 10PB+ data, nightly batch jobs
- Latency: Hours (batch processing acceptable)

---

### Use Case 2: Uber - Ride-Sharing Platform

**OLTP: Trip Management**

- Database: MySQL (partitioned by city)
- Workload: Create trip, update location, calculate fare
- Scale: 15M trips/day, 100K requests/sec
- Latency: <100ms

**OLAP: Business Intelligence**

- Database: Hadoop + Presto
- Workload: Driver performance, pricing optimization, demand forecasting
- Scale: 500TB data, 1000 queries/day
- Latency: 1-10 minutes per query

---

### Use Case 3: Bank - Financial System

**OLTP: Core Banking**

- Database: Oracle RAC (high availability)
- Workload: Deposits, withdrawals, transfers
- Scale: 10M transactions/day
- Latency: <500ms
- Consistency: Strong (ACID critical)

**OLAP: Risk Analysis**

- Database: Teradata / Snowflake
- Workload: Fraud detection, credit scoring, regulatory reports
- Scale: 50TB historical data
- Latency: Minutes to hours
- Consistency: Eventual (daily refresh acceptable)

---

## â“ 10. Frequently Asked Interview Questions

### Q1: What's the main difference between OLTP and OLAP?

**Answer:**

**OLTP (Transaction Processing):**

- Handles operational transactions (orders, payments, updates)
- Row-oriented storage
- Optimized for writes and point lookups
- Normalized schema
- Example: INSERT INTO orders VALUES (...)

**OLAP (Analytical Processing):**

- Handles complex analytical queries (reports, aggregations)
- Column-oriented storage
- Optimized for reads and aggregations
- Denormalized schema (star/snowflake)
- Example: SELECT SUM(revenue) FROM sales GROUP BY month

**Real-world analogy:**

- OLTP = Cash register (fast, frequent, simple transactions)
- OLAP = End-of-year financial report (slow, infrequent, complex analysis)

---

### Q2: Why use column-oriented storage for OLAP?

**Answer:**

**Advantages:**

1. **Read only needed columns:**

```sql
-- Table has 50 columns, query needs 3
SELECT product_name, revenue, date FROM sales;

-- Row storage: Read all 50 columns (wasted I/O)
-- Column storage: Read only 3 columns (17Ã— less I/O) âœ…
```

2. **Better compression:**

```
Column: status â†’ ['paid', 'paid', 'pending', 'paid', 'paid', ...]
Compressed: ['paid': 1,2,4,5], ['pending': 3]
Ratio: 100:1 for low-cardinality columns
```

3. **Vectorized execution:**

```python
# Process 1000 values at once (SIMD)
column_values = [100, 200, 150, 300, ...]
result = sum_vector(column_values)  # CPU parallelism
```

**Trade-off:** Slow for row inserts (must write to multiple column files)

---

### Q3: How do you prevent OLAP queries from killing OLTP performance?

**Answer:**

**Strategies:**

1. **Separate databases:**

```
OLTP (Production) â† Customer requests
OLAP (Warehouse)  â† Analyst queries
```

2. **Read replicas:**

```sql
-- Route analytics to replica
SELECT * FROM orders WHERE created_at >= '2024-01-01'
-- Replica handles load, primary unaffected
```

3. **Query routing:**

```python
if query.is_analytical():
    execute_on_olap_warehouse(query)
else:
    execute_on_oltp_primary(query)
```

4. **Resource limits:**

```sql
SET statement_timeout = '5min';
SET work_mem = '256MB';  -- Prevent memory exhaustion
```

5. **Off-peak scheduling:**

```python
# Run heavy reports at 3am (low traffic)
schedule.every().day.at("03:00").do(generate_reports)
```

---

### Q4: What is a data warehouse and how does it differ from a database?

**Answer:**

| Aspect          | Database (OLTP)         | Data Warehouse (OLAP)     |
| --------------- | ----------------------- | ------------------------- |
| **Purpose**     | Daily operations        | Business intelligence     |
| **Schema**      | Normalized (3NF)        | Star/snowflake (denormal) |
| **Data source** | Application writes      | ETL from multiple sources |
| **Query type**  | Simple, indexed lookups | Complex aggregations      |
| **Users**       | Customers, apps         | Analysts, BI tools        |
| **Data age**    | Current                 | Historical (years)        |
| **Update freq** | Real-time               | Batch (hourly/daily)      |

**Example:**

```
E-commerce Company:
â”‚
â”œâ”€ OLTP Databases:
â”‚  â”œâ”€ Orders DB (MySQL)
â”‚  â”œâ”€ Inventory DB (PostgreSQL)
â”‚  â””â”€ Users DB (MongoDB)
â”‚
â””â”€ Data Warehouse (Snowflake):
   â””â”€ Integrated view of all data
      â””â”€ Analysts run reports across all sources
```

---

### Q5: When would you use HTAP (Hybrid Transaction/Analytical Processing)?

**Answer:**

**HTAP** combines OLTP and OLAP in single system for **real-time analytics**.

**Use cases:**

1. **Real-time dashboards:**

```sql
-- Need up-to-the-second metrics
SELECT COUNT(*) FROM orders WHERE created_at > NOW() - INTERVAL '5 minutes';
-- Can't wait for ETL pipeline (too slow)
```

2. **Fraud detection:**

```sql
-- Analyze transaction patterns in real-time
SELECT AVG(amount), STDDEV(amount)
FROM transactions
WHERE user_id = 123 AND created_at > NOW() - INTERVAL '1 hour';
-- Must detect fraud instantly, not tomorrow
```

3. **Operational analytics:**

```sql
-- Dashboard showing live order volume
SELECT DATE_TRUNC('minute', created_at), COUNT(*)
FROM orders
WHERE created_at > NOW() - INTERVAL '1 hour'
GROUP BY 1;
```

**HTAP Databases:**

- TiDB
- CockroachDB
- SingleStore (MemSQL)
- AlloyDB (Google)

**Trade-off:** More expensive than separate OLTP + OLAP systems, but eliminates ETL lag.

---

## ğŸ§© 11. Design Patterns

### Pattern 1: Lambda Architecture (Batch + Real-time)

**Problem:** Need both historical analytics and real-time insights

**Solution:**

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Data Sources â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
        â”‚ Batch Layer  â”‚      â”‚ Speed Layer â”‚
        â”‚ (OLAP)       â”‚      â”‚ (Real-time) â”‚
        â”‚ Hadoop/Spark â”‚      â”‚ Kafka/Flink â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                â”‚                     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Serving Layer  â”‚
                   â”‚ (Query results)â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Example:**

```python
# Batch layer: Process historical data
def batch_process():
    # Run daily at 2am
    sales_data = spark.read.parquet('s3://data/sales/2024-*')
    aggregated = sales_data.groupBy('date', 'product').sum('revenue')
    aggregated.write.mode('overwrite').parquet('s3://warehouse/sales_daily')

# Speed layer: Process real-time data
def stream_process():
    stream = kafka.subscribe('sales_events')
    stream.groupBy(window('timestamp', '5 minutes'), 'product') \
          .sum('revenue') \
          .writeStream.format('delta').start()

# Serving layer: Merge batch + real-time
def get_sales_data(start_date):
    batch_data = read_from_warehouse(start_date)
    realtime_data = read_from_stream(start_date)
    return merge(batch_data, realtime_data)
```

---

### Pattern 2: Star Schema (Dimensional Modeling)

**Problem:** Optimize data warehouse for analytical queries

**Solution:**

```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  Fact Table      â”‚
                 â”‚  (fact_sales)    â”‚
                 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                 â”‚ date_key    (FK) â”‚â”€â”€â”€â”€â”
                 â”‚ product_key (FK) â”‚â”€â”€â” â”‚
                 â”‚ customer_key(FK) â”‚â”€â”â”‚ â”‚
                 â”‚ store_key   (FK) â”‚â”‚â”‚â”‚
                 â”‚ quantity         â”‚â”‚â”‚â”‚
                 â”‚ revenue          â”‚â”‚â”‚â”‚
                 â”‚ cost             â”‚â”‚â”‚â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚â”‚â”‚
                                     â”‚â”‚â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚â”‚
       â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
       â”‚         â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚         â”‚        â”‚
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â” â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚dim_date â”‚ â”‚dim_prodâ”‚ â”‚  â”‚dim_store â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚date_key â”‚ â”‚prod_keyâ”‚ â”‚  â”‚store_key â”‚
  â”‚date     â”‚ â”‚name    â”‚ â”‚  â”‚name      â”‚
  â”‚month    â”‚ â”‚categoryâ”‚ â”‚  â”‚city      â”‚
  â”‚quarter  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚year     â”‚            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚dim_customer â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚customer_key â”‚
                    â”‚name         â”‚
                    â”‚segment      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**

- âœ… Fast joins (star pattern)
- âœ… Easy to understand
- âœ… Optimized for BI tools

---

## ğŸ“š Summary

**OLTP (Transaction Processing):**

- âœ… Row-oriented, normalized
- âœ… Fast writes, point lookups
- âœ… Current operational data
- âœ… Example: PostgreSQL, MySQL, Oracle

**OLAP (Analytical Processing):**

- âœ… Column-oriented, denormalized
- âœ… Fast aggregations, complex queries
- âœ… Historical analytical data
- âœ… Example: Snowflake, BigQuery, Redshift

**Key Takeaway:** Use the right tool for the job. Don't run analytics on OLTP production databases!

---

**Architecture Pattern:**

```
OLTP (Production) â”€â”€ETLâ”€â”€> OLAP (Warehouse) â”€â”€BI Toolâ”€â”€> Dashboards
```

Keep them separate, sync periodically, query appropriately. âœ…
